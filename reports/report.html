<html><head><title>Report</title></head><body><p>LoftQ, or LoRA-Fine-Tuning-aware Quantization, is a novel framework designed to enhance the performance of quantized pre-trained language models when combined with LoRA fine-tuning. This approach addresses the performance gap often observed between full fine-tuning and the combination of quantization with LoRA fine-tuning. LoftQ achieves this by simultaneously quantizing a large language model and finding an appropriate low-rank initialization for LoRA fine-tuning, which helps to reduce the discrepancy between quantized and full-precision models. This results in improved generalization on downstream tasks such as natural language understanding, question answering, summarization, and natural language generation.

The framework is particularly effective in low-bit scenarios, such as 2-bit and 2/4-bit mixed precision regimes, where it outperforms existing quantization methods. LoftQ integrates low-rank approximation with quantization to better align with the original pre-trained weights, providing a more advantageous starting point for LoRA fine-tuning. This synergy leads to significant improvements in downstream task performance, as demonstrated by experimental results showing LoftQ's superiority over QLoRA across various precision levels.

</p><h2>QLoRA Performance with Different Bits</h2><table border='1'><tr><th>Number of Bits</th><th>Log of Perplexity (a)</th><th>Log of Perplexity (b)</th></tr><tr><td>16</td><td>2.44</td><td>1.0</td></tr><tr><td>8</td><td>2.01</td><td>1.0</td></tr><tr><td>4</td><td>2.23</td><td>1.0</td></tr><tr><td>3</td><td>2.83</td><td>1.0</td></tr><tr><td>2.5</td><td>11.37</td><td>2.82</td></tr><tr><td>2.25</td><td>11.48</td><td>6.40</td></tr><tr><td>2</td><td>11.30</td><td>7.10</td></tr></table></body></html>