<html><head><title>Report</title></head><body><p>LoftQ (LoRA-Fine-Tuning-aware Quantization) is a novel quantization framework designed for large language models (LLMs) that require both quantization and LoRA fine-tuning. It addresses the performance gap observed when quantization and LoRA fine-tuning are applied together on pre-trained models. LoftQ simultaneously quantizes an LLM and finds a suitable low-rank initialization for LoRA fine-tuning, which helps to alleviate the discrepancy between quantized and full-precision models, thereby improving generalization in downstream tasks.

The framework uses an N-bit quantized weight and low-rank approximations to approximate the original high-precision pre-trained weights. This approach provides a promising initialization for LoRA fine-tuning, significantly improving performance in downstream tasks. LoftQ employs alternating optimization between quantization and singular value decomposition (SVD) to minimize the discrepancy between the quantized and original weights.

Experiments demonstrate that LoftQ outperforms existing quantization methods, particularly in low-bit scenarios such as 2-bit and 2/4-bit mixed precision regimes. It has shown superior results in natural language understanding, question answering, summarization, and natural language generation tasks. For instance, LoftQ achieves significant gains in ROUGE scores for summarization tasks and improves accuracy in question answering tasks compared to QLoRA.

LoftQ's method involves storing the quantized weights using an integer matrix and a lookup table, which can be reused for different downstream tasks. The computational cost of LoftQ is minimal as it is applied to individual weight matrices and can be executed in parallel. This makes LoftQ an efficient and effective solution for improving the performance of quantized LLMs in various applications.

</p><h2>QLoRA Performance with Different Bits</h2><table border='1'><tr><th>Number of Bits</th><th>Log of Perplexity (Pre-trained LLAMA-2-13b on WikiText-2)</th></tr><tr><td>16</td><td>2.44</td></tr><tr><td>8</td><td>2.01</td></tr><tr><td>4</td><td>2.23</td></tr><tr><td>3</td><td>2.83</td></tr><tr><td>2.5</td><td>11.37</td></tr><tr><td>2.25</td><td>11.48</td></tr><tr><td>2</td><td>11.36</td></tr></table><h2>QLoRA Performance with Different Bits</h2><table border='1'><tr><th>Number of Bits</th><th>Log of Perplexity (Fine-tuned LLAMA-2-13b on WikiText-2)</th></tr><tr><td>16</td><td>1.63</td></tr><tr><td>8</td><td>1.64</td></tr><tr><td>4</td><td>1.63</td></tr><tr><td>3</td><td>1.63</td></tr><tr><td>2.5</td><td>2.82</td></tr><tr><td>2.25</td><td>6.40</td></tr><tr><td>2</td><td>6.37</td></tr></table></body></html>